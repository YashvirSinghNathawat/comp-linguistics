{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-13T03:03:36.498618Z",
     "iopub.status.busy": "2024-03-13T03:03:36.497944Z",
     "iopub.status.idle": "2024-03-13T03:04:41.152273Z",
     "shell.execute_reply": "2024-03-13T03:04:41.151172Z",
     "shell.execute_reply.started": "2024-03-13T03:03:36.498587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=3a8b0cc9da176747be2e9aa398bfb95893dc8d131978f0ab8d52f03a8b6f4a23\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.38.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.27.2)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.27.2\n",
      "    Uninstalling accelerate-0.27.2:\n",
      "      Successfully uninstalled accelerate-0.27.2\n",
      "Successfully installed accelerate-0.28.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install sentencepiece\n",
    "!pip3 install seqeval\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:04:41.155474Z",
     "iopub.status.busy": "2024-03-13T03:04:41.154642Z",
     "iopub.status.idle": "2024-03-13T03:04:42.615517Z",
     "shell.execute_reply": "2024-03-13T03:04:42.614618Z",
     "shell.execute_reply.started": "2024-03-13T03:04:41.155436Z"
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:04:42.617249Z",
     "iopub.status.busy": "2024-03-13T03:04:42.616698Z",
     "iopub.status.idle": "2024-03-13T03:08:10.724640Z",
     "shell.execute_reply": "2024-03-13T03:08:10.723907Z",
     "shell.execute_reply.started": "2024-03-13T03:04:42.617217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40286aa8a6a477282dcac78b84f1931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset naamapadam_pr/hi to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/hi/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230f73d5b4084b27b4531621b718f76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/82.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset naamapadam_pr downloaded and prepared to /root/.cache/huggingface/datasets/ai4bharat___naamapadam_pr/hi/1.0.0/99b5ec77eabfaa3fbff510d8cf70d7c34519486cb7dbee99ede19474ddff9b20. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Let's download the Naampadam (Indic NER) dataset\n",
    "from datasets import ClassLabel, load_dataset, load_metric, DownloadMode\n",
    "\n",
    "lang='hi'\n",
    "\n",
    "train_dataset = load_dataset('ai4bharat/naamapadam', lang , split = 'train[:20000]')\n",
    "validation_dataset = load_dataset('ai4bharat/naamapadam', lang , split = 'validation')\n",
    "test_dataset = load_dataset('ai4bharat/naamapadam', lang , split = 'test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:10.726921Z",
     "iopub.status.busy": "2024-03-13T03:08:10.726654Z",
     "iopub.status.idle": "2024-03-13T03:08:10.731552Z",
     "shell.execute_reply": "2024-03-13T03:08:10.730666Z",
     "shell.execute_reply.started": "2024-03-13T03:08:10.726897Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=7, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC'], id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "features = train_dataset.features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:10.732921Z",
     "iopub.status.busy": "2024-03-13T03:08:10.732622Z",
     "iopub.status.idle": "2024-03-13T03:08:10.743466Z",
     "shell.execute_reply": "2024-03-13T03:08:10.742572Z",
     "shell.execute_reply.started": "2024-03-13T03:08:10.732899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}\n"
     ]
    }
   ],
   "source": [
    "text_column_name = \"tokens\"\n",
    "label_column_name = \"ner_tags\"\n",
    "\n",
    "label_list = features[label_column_name].feature.names\n",
    "\n",
    "label_to_id = {label_list[i]: features[label_column_name].feature.str2int( label_list[i] ) for i in range(len(label_list))}\n",
    "\n",
    "print(label_to_id)\n",
    "\n",
    "num_labels = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:10.744831Z",
     "iopub.status.busy": "2024-03-13T03:08:10.744575Z",
     "iopub.status.idle": "2024-03-13T03:08:44.403834Z",
     "shell.execute_reply": "2024-03-13T03:08:44.403086Z",
     "shell.execute_reply.started": "2024-03-13T03:08:10.744809Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 03:08:18.952405: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-13 03:08:18.952502: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-13 03:08:19.078161: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad9ff527e764bb98518eec546c5cdc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b432b3ed7142e19f091b4e6aaa4c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/5.65M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b1e7cfc7fd412087f355b085aef095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/135M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of AlbertForTokenClassification were not initialized from the model checkpoint at ai4bharat/indic-bert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\n",
    "import numpy as np\n",
    "\n",
    "config = AutoConfig.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels, finetuning_task='ner')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\n",
    "model = AutoModelForTokenClassification.from_pretrained('ai4bharat/indic-bert', num_labels=num_labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:44.405455Z",
     "iopub.status.busy": "2024-03-13T03:08:44.404916Z",
     "iopub.status.idle": "2024-03-13T03:08:44.413006Z",
     "shell.execute_reply": "2024-03-13T03:08:44.412091Z",
     "shell.execute_reply.started": "2024-03-13T03:08:44.405430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize all texts and align the labels with them.\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    # print(tokenized_inputs)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        # print('=====')\n",
    "        # print('{} {}'.format(i,label)) #ak\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    # print(tokenized_inputs)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:44.414792Z",
     "iopub.status.busy": "2024-03-13T03:08:44.414232Z",
     "iopub.status.idle": "2024-03-13T03:08:44.429336Z",
     "shell.execute_reply": "2024-03-13T03:08:44.428463Z",
     "shell.execute_reply.started": "2024-03-13T03:08:44.414761Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁सक', 'टर', '▁55', '/', '56', '▁क', '▁एसएचओ', '▁अर', 'वद', '▁कमर', '▁न', '▁बत', 'य', '▁क', '▁इस', '▁म', 'मल', '▁म', '▁आई', 'पस', '▁क', '▁धर', '▁376', '▁-', '▁ड', '▁(', '▁ग', 'गर', 'प', '▁)', '▁क', '▁तहत', '▁म', 'मल', '▁दर', 'ज', '▁कर', '▁लय', '▁ग', 'य', '▁ह', '▁।', '[SEP]', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "correct_output = tokenize_and_align_labels(train_dataset[0:1])\n",
    "tokens = tokenizer.convert_ids_to_tokens(correct_output[\"input_ids\"][0])\n",
    "print(tokens)\n",
    "\n",
    "# Now we have input ids and labels to pass to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:44.430632Z",
     "iopub.status.busy": "2024-03-13T03:08:44.430356Z",
     "iopub.status.idle": "2024-03-13T03:08:44.437808Z",
     "shell.execute_reply": "2024-03-13T03:08:44.436958Z",
     "shell.execute_reply.started": "2024-03-13T03:08:44.430603Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]___________________________________-100\n",
      "▁सक_____________________________________0\n",
      "टर______________________________________-100\n",
      "▁55_____________________________________0\n",
      "/_______________________________________-100\n",
      "56______________________________________-100\n",
      "▁क______________________________________0\n",
      "▁एसएचओ__________________________________0\n",
      "▁अर_____________________________________1\n",
      "वद______________________________________-100\n",
      "▁कमर____________________________________2\n",
      "▁न______________________________________0\n",
      "▁बत_____________________________________0\n",
      "य_______________________________________-100\n",
      "▁क______________________________________0\n",
      "▁इस_____________________________________0\n",
      "▁म______________________________________0\n",
      "मल______________________________________-100\n",
      "▁म______________________________________0\n",
      "▁आई_____________________________________0\n",
      "पस______________________________________-100\n",
      "▁क______________________________________0\n",
      "▁धर_____________________________________0\n",
      "▁376____________________________________0\n",
      "▁-______________________________________0\n",
      "▁ड______________________________________0\n",
      "▁(______________________________________0\n",
      "▁ग______________________________________0\n",
      "गर______________________________________-100\n",
      "प_______________________________________-100\n",
      "▁)______________________________________0\n",
      "▁क______________________________________0\n",
      "▁तहत____________________________________0\n",
      "▁म______________________________________0\n",
      "मल______________________________________-100\n",
      "▁दर_____________________________________0\n",
      "ज_______________________________________-100\n",
      "▁कर_____________________________________0\n",
      "▁लय_____________________________________0\n",
      "▁ग______________________________________0\n",
      "य_______________________________________-100\n",
      "▁ह______________________________________0\n",
      "▁।______________________________________0\n",
      "[SEP]___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n",
      "<pad>___________________________________-100\n"
     ]
    }
   ],
   "source": [
    "for token,label in zip(tokenizer.convert_ids_to_tokens(correct_output[\"input_ids\"][0]) , correct_output[\"labels\"][0]):\n",
    "  print(f\"{token:_<40}{label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:44.441077Z",
     "iopub.status.busy": "2024-03-13T03:08:44.440736Z",
     "iopub.status.idle": "2024-03-13T03:08:53.162387Z",
     "shell.execute_reply": "2024-03-13T03:08:53.161463Z",
     "shell.execute_reply.started": "2024-03-13T03:08:44.441035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9d3eb10d949b78b4ad7d209b3a4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 20000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")\n",
    "print(tokenized_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:53.163794Z",
     "iopub.status.busy": "2024-03-13T03:08:53.163472Z",
     "iopub.status.idle": "2024-03-13T03:08:53.172032Z",
     "shell.execute_reply": "2024-03-13T03:08:53.171175Z",
     "shell.execute_reply.started": "2024-03-13T03:08:53.163770Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "256\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "# Problem is fixed now!!\n",
    "print(len(tokenized_train_dataset[0]['ner_tags']))\n",
    "print(len(tokenized_train_dataset[0]['labels']))\n",
    "print(len(tokenized_train_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:53.174507Z",
     "iopub.status.busy": "2024-03-13T03:08:53.173399Z",
     "iopub.status.idle": "2024-03-13T03:08:53.825682Z",
     "shell.execute_reply": "2024-03-13T03:08:53.824785Z",
     "shell.execute_reply.started": "2024-03-13T03:08:53.174481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960e0cdf7dc44358172939ae7d28b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 867\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Test Dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")\n",
    "print(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:08:53.828807Z",
     "iopub.status.busy": "2024-03-13T03:08:53.828461Z",
     "iopub.status.idle": "2024-03-13T03:09:00.016120Z",
     "shell.execute_reply": "2024-03-13T03:09:00.015182Z",
     "shell.execute_reply.started": "2024-03-13T03:08:53.828784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11a901f9e9104d9eb45a55ce77ee2ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 13460\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Validation DataSet\n",
    "tokenized_validation_dataset = validation_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")\n",
    "print(tokenized_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:00.018013Z",
     "iopub.status.busy": "2024-03-13T03:09:00.017333Z",
     "iopub.status.idle": "2024-03-13T03:09:00.022208Z",
     "shell.execute_reply": "2024-03-13T03:09:00.021234Z",
     "shell.execute_reply.started": "2024-03-13T03:09:00.017976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:00.023507Z",
     "iopub.status.busy": "2024-03-13T03:09:00.023205Z",
     "iopub.status.idle": "2024-03-13T03:09:01.370974Z",
     "shell.execute_reply": "2024-03-13T03:09:01.370286Z",
     "shell.execute_reply.started": "2024-03-13T03:09:00.023484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfa60e4eb374440966dfc957d037843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Metric to see how my model is performing For NER we use seqeval\n",
    "metric = datasets.load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:01.372347Z",
     "iopub.status.busy": "2024-03-13T03:09:01.372076Z",
     "iopub.status.idle": "2024-03-13T03:09:01.388970Z",
     "shell.execute_reply": "2024-03-13T03:09:01.387984Z",
     "shell.execute_reply.started": "2024-03-13T03:09:01.372323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing the working of seqeval\n",
    "example_text = train_dataset[0]\n",
    "label_list = train_dataset.features[\"ner_tags\"].feature.names\n",
    "label_list\n",
    "\n",
    "labels = [label_list[i] for i in example_text[\"ner_tags\"]]\n",
    "print(labels)\n",
    "\n",
    "metric.compute(predictions=[labels] , references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:01.390493Z",
     "iopub.status.busy": "2024-03-13T03:09:01.390227Z",
     "iopub.status.idle": "2024-03-13T03:09:02.249255Z",
     "shell.execute_reply": "2024-03-13T03:09:02.248490Z",
     "shell.execute_reply.started": "2024-03-13T03:09:01.390470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    # Unpack nested dictionaries\n",
    "    final_results = {}\n",
    "    for key, value in results.items():\n",
    "        if isinstance(value, dict):\n",
    "            for n, v in value.items():\n",
    "                final_results[f\"{key}_{n}\"] = v\n",
    "        else:\n",
    "            final_results[key] = value\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:02.250554Z",
     "iopub.status.busy": "2024-03-13T03:09:02.250312Z",
     "iopub.status.idle": "2024-03-13T03:09:02.307002Z",
     "shell.execute_reply": "2024-03-13T03:09:02.306274Z",
     "shell.execute_reply.started": "2024-03-13T03:09:02.250533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define Training Args\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"model/upos\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    "#     weight_decay=0.001,\n",
    "    save_total_limit=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:02.308218Z",
     "iopub.status.busy": "2024-03-13T03:09:02.307943Z",
     "iopub.status.idle": "2024-03-13T03:09:03.304191Z",
     "shell.execute_reply": "2024-03-13T03:09:03.303443Z",
     "shell.execute_reply.started": "2024-03-13T03:09:02.308194Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "   model,\n",
    "   args,\n",
    "   train_dataset=tokenized_train_dataset,\n",
    "   eval_dataset=tokenized_validation_dataset,\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T03:09:03.305912Z",
     "iopub.status.busy": "2024-03-13T03:09:03.305359Z",
     "iopub.status.idle": "2024-03-13T03:57:04.740063Z",
     "shell.execute_reply": "2024-03-13T03:57:04.738789Z",
     "shell.execute_reply.started": "2024-03-13T03:09:03.305877Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240313_031756-pqohmyfj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/desixgod/huggingface/runs/pqohmyfj' target=\"_blank\">chocolate-wood-4</a></strong> to <a href='https://wandb.ai/desixgod/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/desixgod/huggingface' target=\"_blank\">https://wandb.ai/desixgod/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/desixgod/huggingface/runs/pqohmyfj' target=\"_blank\">https://wandb.ai/desixgod/huggingface/runs/pqohmyfj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 38:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Loc Precision</th>\n",
       "      <th>Loc Recall</th>\n",
       "      <th>Loc F1</th>\n",
       "      <th>Loc Number</th>\n",
       "      <th>Org Precision</th>\n",
       "      <th>Org Recall</th>\n",
       "      <th>Org F1</th>\n",
       "      <th>Org Number</th>\n",
       "      <th>Per Precision</th>\n",
       "      <th>Per Recall</th>\n",
       "      <th>Per F1</th>\n",
       "      <th>Per Number</th>\n",
       "      <th>Overall Precision</th>\n",
       "      <th>Overall Recall</th>\n",
       "      <th>Overall F1</th>\n",
       "      <th>Overall Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.513065</td>\n",
       "      <td>0.585958</td>\n",
       "      <td>0.222266</td>\n",
       "      <td>0.322283</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.400157</td>\n",
       "      <td>0.104231</td>\n",
       "      <td>0.165383</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.340707</td>\n",
       "      <td>0.319171</td>\n",
       "      <td>0.329588</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.408197</td>\n",
       "      <td>0.217980</td>\n",
       "      <td>0.284197</td>\n",
       "      <td>0.848279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.454900</td>\n",
       "      <td>0.426036</td>\n",
       "      <td>0.600458</td>\n",
       "      <td>0.411143</td>\n",
       "      <td>0.488086</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.402412</td>\n",
       "      <td>0.242081</td>\n",
       "      <td>0.302303</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.637253</td>\n",
       "      <td>0.382002</td>\n",
       "      <td>0.477667</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.551913</td>\n",
       "      <td>0.346943</td>\n",
       "      <td>0.426058</td>\n",
       "      <td>0.873923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.404500</td>\n",
       "      <td>0.386344</td>\n",
       "      <td>0.592967</td>\n",
       "      <td>0.528346</td>\n",
       "      <td>0.558795</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.471468</td>\n",
       "      <td>0.276926</td>\n",
       "      <td>0.348912</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.574091</td>\n",
       "      <td>0.524603</td>\n",
       "      <td>0.548232</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.557029</td>\n",
       "      <td>0.446560</td>\n",
       "      <td>0.495715</td>\n",
       "      <td>0.883868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.356900</td>\n",
       "      <td>0.365561</td>\n",
       "      <td>0.681665</td>\n",
       "      <td>0.554783</td>\n",
       "      <td>0.611714</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.397332</td>\n",
       "      <td>0.450439</td>\n",
       "      <td>0.422222</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.662432</td>\n",
       "      <td>0.483535</td>\n",
       "      <td>0.559020</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.559882</td>\n",
       "      <td>0.496745</td>\n",
       "      <td>0.526427</td>\n",
       "      <td>0.888860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.348000</td>\n",
       "      <td>0.350921</td>\n",
       "      <td>0.630350</td>\n",
       "      <td>0.602761</td>\n",
       "      <td>0.616247</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.523181</td>\n",
       "      <td>0.356325</td>\n",
       "      <td>0.423926</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.596521</td>\n",
       "      <td>0.590651</td>\n",
       "      <td>0.593572</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.590630</td>\n",
       "      <td>0.519678</td>\n",
       "      <td>0.552887</td>\n",
       "      <td>0.895331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.339974</td>\n",
       "      <td>0.709629</td>\n",
       "      <td>0.560658</td>\n",
       "      <td>0.626408</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.455990</td>\n",
       "      <td>0.466381</td>\n",
       "      <td>0.461127</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.594867</td>\n",
       "      <td>0.620647</td>\n",
       "      <td>0.607484</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.578924</td>\n",
       "      <td>0.551215</td>\n",
       "      <td>0.564730</td>\n",
       "      <td>0.896606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.323900</td>\n",
       "      <td>0.331232</td>\n",
       "      <td>0.644712</td>\n",
       "      <td>0.629688</td>\n",
       "      <td>0.637111</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.530648</td>\n",
       "      <td>0.402514</td>\n",
       "      <td>0.457784</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.644785</td>\n",
       "      <td>0.596707</td>\n",
       "      <td>0.619815</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.613585</td>\n",
       "      <td>0.545556</td>\n",
       "      <td>0.577574</td>\n",
       "      <td>0.900661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.326960</td>\n",
       "      <td>0.649739</td>\n",
       "      <td>0.633898</td>\n",
       "      <td>0.641721</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.524230</td>\n",
       "      <td>0.431126</td>\n",
       "      <td>0.473141</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.659737</td>\n",
       "      <td>0.597559</td>\n",
       "      <td>0.627110</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.616589</td>\n",
       "      <td>0.556417</td>\n",
       "      <td>0.584960</td>\n",
       "      <td>0.902065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.303900</td>\n",
       "      <td>0.322878</td>\n",
       "      <td>0.661470</td>\n",
       "      <td>0.627338</td>\n",
       "      <td>0.643952</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.511372</td>\n",
       "      <td>0.441140</td>\n",
       "      <td>0.473667</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.653760</td>\n",
       "      <td>0.610333</td>\n",
       "      <td>0.631301</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.613489</td>\n",
       "      <td>0.561848</td>\n",
       "      <td>0.586534</td>\n",
       "      <td>0.902669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>0.321350</td>\n",
       "      <td>0.656127</td>\n",
       "      <td>0.635954</td>\n",
       "      <td>0.645883</td>\n",
       "      <td>10213</td>\n",
       "      <td>0.508409</td>\n",
       "      <td>0.444819</td>\n",
       "      <td>0.474493</td>\n",
       "      <td>9786</td>\n",
       "      <td>0.661991</td>\n",
       "      <td>0.607305</td>\n",
       "      <td>0.633470</td>\n",
       "      <td>10568</td>\n",
       "      <td>0.613226</td>\n",
       "      <td>0.564858</td>\n",
       "      <td>0.588049</td>\n",
       "      <td>0.902870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T19:21:56.250125Z",
     "iopub.status.busy": "2024-03-12T19:21:56.249430Z",
     "iopub.status.idle": "2024-03-12T19:21:56.256781Z",
     "shell.execute_reply": "2024-03-12T19:21:56.255624Z",
     "shell.execute_reply.started": "2024-03-12T19:21:56.250091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2388.4194, 'train_samples_per_second': 25.121, 'train_steps_per_second': 1.57, 'total_flos': 662868449280000.0, 'train_loss': 0.3149069254557292, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T19:21:58.521952Z",
     "iopub.status.busy": "2024-03-12T19:21:58.521600Z",
     "iopub.status.idle": "2024-03-12T19:21:58.528647Z",
     "shell.execute_reply": "2024-03-12T19:21:58.527545Z",
     "shell.execute_reply.started": "2024-03-12T19:21:58.521928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.720911\n"
     ]
    }
   ],
   "source": [
    "sumoff1score=0.776407+0.624282+0.762044\n",
    "macrof1=sumoff1score/3\n",
    "print(macrof1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T19:21:59.298909Z",
     "iopub.status.busy": "2024-03-12T19:21:59.298422Z",
     "iopub.status.idle": "2024-03-12T19:21:59.643119Z",
     "shell.execute_reply": "2024-03-12T19:21:59.642026Z",
     "shell.execute_reply.started": "2024-03-12T19:21:59.298877Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save Model\n",
    "trainer.save_model(\"model_indic_ner_5Epoch_one_lakh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "from IPython.display import FileLink\n",
    "\n",
    "def zip_dir(directory = os.curdir, file_name = 'directory.zip'):\n",
    "    \"\"\"\n",
    "    zip all the files in a directory\n",
    "    \n",
    "    Parameters\n",
    "    ___\n",
    "    directory: str\n",
    "        directory needs to be zipped, defualt is current working directory\n",
    "        \n",
    "    file_name: str\n",
    "        the name of the zipped file (including .zip), default is 'directory.zip'\n",
    "        \n",
    "    Returns\n",
    "    ___\n",
    "    Creates a hyperlink, which can be used to download the zip file)\n",
    "    \"\"\"\n",
    "    os.chdir(directory)\n",
    "    zip_ref = zipfile.ZipFile(file_name, mode='w')\n",
    "    for folder, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file_name in file:\n",
    "                pass\n",
    "            else:\n",
    "                zip_ref.write(os.path.join(folder, file))\n",
    "\n",
    "    return FileLink(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-12T19:26:43.206492Z",
     "iopub.status.busy": "2024-03-12T19:26:43.205788Z",
     "iopub.status.idle": "2024-03-12T19:26:43.217181Z",
     "shell.execute_reply": "2024-03-12T19:26:43.216161Z",
     "shell.execute_reply.started": "2024-03-12T19:26:43.206460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.2399081140756607\n",
      "test_LOC_precision: 0.6621848739495798\n",
      "test_LOC_recall: 0.6416938110749185\n",
      "test_LOC_f1: 0.6517783291976841\n",
      "test_LOC_number: 614\n",
      "test_ORG_precision: 0.5900383141762452\n",
      "test_ORG_recall: 0.5866666666666667\n",
      "test_ORG_f1: 0.5883476599808979\n",
      "test_ORG_number: 525\n",
      "test_PER_precision: 0.7523302263648469\n",
      "test_PER_recall: 0.7151898734177216\n",
      "test_PER_f1: 0.7332900713822195\n",
      "test_PER_number: 790\n",
      "test_overall_precision: 0.6782655246252677\n",
      "test_overall_recall: 0.6568170036288232\n",
      "test_overall_f1: 0.6673689755069792\n",
      "test_overall_accuracy: 0.9253003569094657\n",
      "test_runtime: 8.246\n",
      "test_samples_per_second: 105.142\n",
      "test_steps_per_second: 6.67\n"
     ]
    }
   ],
   "source": [
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on 20% Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T04:03:12.081537Z",
     "iopub.status.busy": "2024-03-13T04:03:12.080808Z",
     "iopub.status.idle": "2024-03-13T04:03:14.071692Z",
     "shell.execute_reply": "2024-03-13T04:03:14.070542Z",
     "shell.execute_reply.started": "2024-03-13T04:03:12.081507Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d27f1712924cfb830cb08a9decabd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 867\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel, load_dataset, load_metric, DownloadMode\n",
    "lang = 'hi'\n",
    "test_dataset = load_dataset('ai4bharat/naamapadam', lang , split = 'test')\n",
    "# Test Dataset\n",
    "tokenized_test_dataset = test_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")\n",
    "print(tokenized_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-13T04:03:14.074100Z",
     "iopub.status.busy": "2024-03-13T04:03:14.073698Z",
     "iopub.status.idle": "2024-03-13T04:03:22.562637Z",
     "shell.execute_reply": "2024-03-13T04:03:22.561593Z",
     "shell.execute_reply.started": "2024-03-13T04:03:14.074037Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Test metrics *****\n",
      "  test_LOC_f1             =     0.6044\n",
      "  test_LOC_number         =        614\n",
      "  test_LOC_precision      =     0.6254\n",
      "  test_LOC_recall         =     0.5847\n",
      "  test_ORG_f1             =     0.5068\n",
      "  test_ORG_number         =        525\n",
      "  test_ORG_precision      =     0.5168\n",
      "  test_ORG_recall         =     0.4971\n",
      "  test_PER_f1             =      0.668\n",
      "  test_PER_number         =        790\n",
      "  test_PER_precision      =     0.7025\n",
      "  test_PER_recall         =     0.6367\n",
      "  test_loss               =      0.277\n",
      "  test_overall_accuracy   =     0.9122\n",
      "  test_overall_f1         =     0.6031\n",
      "  test_overall_precision  =     0.6256\n",
      "  test_overall_recall     =     0.5822\n",
      "  test_runtime            = 0:00:08.47\n",
      "  test_samples_per_second =    102.331\n",
      "  test_steps_per_second   =     12.865\n",
      "\n",
      " Macro f1 score:: 0.5930559695989885\n"
     ]
    }
   ],
   "source": [
    "# Evaluation of Model on test Data\n",
    "predictions, labels, metrics = trainer.predict(tokenized_test_dataset)\n",
    "trainer.log_metrics(\"Test\",metrics)\n",
    "\n",
    "sum_of_f1_scores=metrics['test_LOC_f1']+metrics['test_ORG_f1']+metrics['test_PER_f1']\n",
    "macro_f1=sum_of_f1_scores/3\n",
    "print('\\n Macro f1 score::',macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30665,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
